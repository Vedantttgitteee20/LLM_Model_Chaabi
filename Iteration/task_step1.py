# -*- coding: utf-8 -*-
"""task_step1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OTAO0pnn9MtqViYzEO6RvnYFaYdJRdh9
"""

import pandas as pd
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from gensim.parsing.preprocessing import remove_stopwords
import string

import nltk

# Uncomment the line below and run it once to download the necessary data
# nltk.download('punkt')

# Load CSV file into a DataFrame
df = pd.read_csv("bigBasketProducts.csv")
df

# Assuming 'description' is the column containing product descriptions
# Fill NaN values with an empty string
df['description'] = df['description'].fillna("")
df

import nltk
nltk.download('punkt')

# Tokenize and preprocess the product descriptions
tokenized_descriptions = [word_tokenize(remove_stopwords(str(description).lower())) for description in df['description']]

# Remove punctuation from tokens
translator = str.maketrans("", "", string.punctuation)
tokenized_descriptions = [[token.translate(translator) for token in tokens] for tokens in tokenized_descriptions]

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=tokenized_descriptions, vector_size=100, window=5, min_count=1)

# Get vector for a specific description (example)
sample_description = "This Product contains Garlic Oil that is known to help proper digestion, maintain proper cholesterol levels, support cardiovascular and also build immunity."
cleaned_sample_tokens = [token.translate(translator) for token in word_tokenize(remove_stopwords(sample_description.lower()))]
vector_for_description = word2vec_model.wv[cleaned_sample_tokens]

# Save the Word2Vec model for future use
word2vec_model.save("word2vec_model.model")

# Save vectors to CSV file
vectors_df = pd.DataFrame(word2vec_model.wv.vectors, columns=[f"dim_{i}" for i in range(100)])
vectors_df.to_csv("vectors.csv", index=False)

# import subprocess

# # Replace 'package_name' with the name of the package you want to install
# package_name = 'transformers'

# try:
#     # Execute pip install command
#     subprocess.check_call(["pip", "install", package_name])
#     print(f"Successfully installed {package_name}")
# except subprocess.CalledProcessError:
#     print(f"Error installing {package_name}")

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained GPT-2 model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

from flask import Flask, request, jsonify
import numpy as np
from scipy.spatial.distance import cdist

app = Flask(__name__)

# Load vectors from CSV file
vectors_df = pd.read_csv("vectors.csv")

@app.route("/get_answer", methods=["POST"])
def get_answer():
    data = request.get_json()
    user_query = data["query"]

    # Convert user query to vector using Word2Vec
    cleaned_query_tokens = [token.translate(translator) for token in word_tokenize(remove_stopwords(user_query.lower()))]
    user_query_vector = np.mean([word2vec_model.wv[token] for token in cleaned_query_tokens if token in word2vec_model.wv], axis=0)

    # Find the most similar vector using cosine similarity
    similarities = 1 - cdist([user_query_vector], vectors_df.values, 'cosine')
    most_similar_index = np.argmax(similarities)

    # Get the corresponding description
    most_similar_description = df.loc[most_similar_index, 'description']

    # Generate a response using GPT-2
    input_ids = tokenizer.encode(user_query, return_tensors="pt")
    output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)
    generated_response = tokenizer.decode(output[0], skip_special_tokens=True)

    return jsonify({"most_similar_description": most_similar_description, "generated_response": generated_response})

if __name__ == "__main__":
    app.run(debug=True)

!pip install pyngrok

from pyngrok import ngrok

# Set up ngrok with your authtoken
ngrok.set_auth_token("2YLuiMQcKywrqpAV36KKpdqmaUc_6x5KsNhgyFNepX9KB4F9y")